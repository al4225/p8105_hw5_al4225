---
title: "p8105_hw5_al4225"
author: "Anjing"
date: "2022-11-16"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(broom)
library(dplyr)
knitr::opts_chunk$set(
  fig.width = 7,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 2
### Raw data
**Describe the raw data.**    
In this raw dataset, it contains information about homicides in 50 large U.S. cities. There are `r homicide_data = read_csv("./data/homicide-data.csv",
                col_names = TRUE) %>%
  janitor::clean_names() 
  nrow(homicide_data)` observations and `r homicide_data = read_csv("./data/homicide-data.csv",
                col_names = TRUE) %>%
  janitor::clean_names()
  ncol(homicide_data)` variables. Some key variables such like victim_last, victim_first, victim_race, victim_age, victim_sex discribe victims' basic information about their name, race, age and sex. Some key variables such like reported_date, city, state, lat, lon describe the basic information about the date and location of homicides in U.S.. The variable disposition reflects the disposition of the murders.
```{r}
homicide_data = read_csv("./data/homicide-data.csv",
                col_names = TRUE) %>%
  janitor::clean_names() 
homicide_data
```


### Homicides
Create a city_state variable (e.g. “Baltimore, MD”) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

**Discriptions**:     
First, I created a city_state variable to show the summarized information of location like “Baltimore, MD”. Then I noticed that one homocide‘s city_state is wrong which is "Tulsa, AL" so I delete this row because we don’t know whether the city name or the state name is wrong. Then I summarized the total number of homicides and the number of unsolved homicides by cities.     
The total number of homicides of all cities is `r homicide = 
  homicide_data %>%
  mutate(
    city_state = str_c(city, ", ", state)) %>%
  mutate(disposition_situation = ifelse(disposition %in% c("Closed without arrest", "Open/No arrest"), "unsolved", "solved")) %>%
  filter(city_state != "Tulsa, AL")
  nrow(homicide)`. The total number of unsolved homicides in all cities is `r homicide %>%
  filter(disposition_situation == "unsolved") %>%
  nrow()`. In each city, the total and unsolved number of homicides in each city are shown below. We can see that Chicago, IL has the largest total(5535) and unsolved(4073) number of homicides so the city is relatively dangeous. Tampa, FL has the minimum total(208) and unsolved(95) number of homicides so the city is relatively safe.
```{r}
homicide = 
  homicide_data %>%
  mutate(
    city_state = str_c(city, ", ", state)) %>%
  mutate(disposition_situation = ifelse(disposition %in% c("Closed without arrest", "Open/No arrest"), "unsolved", "solved")) %>%
  filter(city_state != "Tulsa, AL")

nrow(homicide)

total_homicide = 
homicide %>%
  filter(disposition_situation == "unsolved") %>%
  nrow()
total_homicide

total_homicides =
  homicide %>%
  group_by(city_state) %>%  
  summarize(
    n_obs = n()) %>%
  arrange(desc(n_obs))
total_homicides

unsoved_homicides = 
  homicide %>%
  filter(disposition_situation == "unsolved") %>%
  group_by(city_state) %>%  
  summarize(
    n_obs = n()) %>%
  arrange(desc(n_obs))
unsoved_homicides
```


### prop.test
For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.    
**Description**:    
For Baltimore, MD, the estimated proportion of homicides that are unsolved is 0.6455607. The confidence interval is (0.6275625, 0.6631599) in α=0.05 level which means that we are 95% confident that the true proportion of homicides that are unsolved lies in this interval. It concludes the estimated value.
```{r}
Baltimore = 
homicide %>%
  filter(city_state == "Baltimore, MD")

total_unsolved_Baltimore = 
  Baltimore %>%
  summarise(
    unsolved = sum(disposition_situation == "unsolved"),
    n = n()
  )

total_unsolved_Baltimore

prop_test = 
  prop.test(
    x = total_unsolved_Baltimore %>% pull(unsolved),
    n = total_unsolved_Baltimore %>% pull(n)
  )
prop_test

prop_test %>%
  broom::tidy()
```


### prop.test for each city
Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.
```{r}
total_unsolved_cities = 
  homicide %>%
  group_by(city_state) %>% 
  summarise(
    unsolved_cities = sum(disposition_situation == "unsolved"),
    n_cities = n()
  )

total_unsolved_cities
```

**Descriptions:**   
This dataset reflects each city's estimated proportion of unsolved homicides and the confidence interval.
```{r}
results_cities = 
  total_unsolved_cities %>% 
  mutate(
    prop_tests = map2(.x = unsolved_cities, .y = n_cities, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(city_state, tidy_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)
results_cities
```

### Plots
Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.     
**Description**:    
In this plot, we can see the proportion of unsolved homicides in each city from high to low and the error bar in each point. The overall proportions of unsolved homicides are between 0.25 and 0.75. Chicago has the highest proportion of unsolved homicides which reflects the unsafety of this city in a way and Richmond has the lowest proportion of unsolved homicides which reflects the safety of this city relatively.
```{r results_cities_plot}
results_cities_plot = 
results_cities %>% 
  mutate(city_state = fct_reorder(city_state, estimate, .desc = TRUE)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  geom_smooth(se = TRUE) +
  labs(
    title = "Proportion of unsolved homicides by cities",
    x = "City_state",
    y = "Estimated proportion of unsolved homicides",
    caption = "Data from Washington Post "
  ) +
  theme(legend.position = "bottom") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5),
        plot.title = element_text(hjust = 0.5))
results_cities_plot
```

## Problem 3
When designing an experiment or analysis, a common question is whether it is likely that a true effect will be detected – put differently, whether a false null hypothesis will be rejected. The probability that a false null hypothesis is rejected is referred to as power, and it depends on several factors, including: the sample size; the effect size; and the error variance. In this problem, you will conduct a simulation to explore power in a one-sample t-test.  

### the t-test
First set the following design elements:   

Fix n=30   
Fix σ=5   
Set μ=0. Generate 5000 datasets from the model.   
x∼Normal[μ,σ]   

For each dataset, save μ^ and the p-value arising from a test of H:μ=0 using α=0.05. Hint: to obtain the estimate and p-value, use broom::tidy to clean the output of t.test.   
Repeat the above for μ={1,2,3,4,5,6}.    

**Description**:   
First, I write a function to conduct the t-test of norm and map μ = 0,1,2,3,4,5,6 to this function then we get the p-value summary table below.
```{r}
sim_t_test = 
  function(true_mean){
    sample = tibble(rnorm(30, mean = true_mean, sd = 5))
    
    test_result = t.test(sample)
    
    test_result %>%
      broom::tidy()
  }

sim_results_df = 
  expand_grid(
    true_mean = 0:6,
    iter = 1:5000) %>%
  mutate(estimate_df = map(true_mean, sim_t_test)) %>%
  unnest(estimate_df) %>%
  select(true_mean, iter, estimate, p.value) %>%
  mutate(test_result = ifelse(p.value < 0.05, "reject H0", "fail to reject H0"))
sim_results_df
```

### True_mean-power plot
The proportions of times the null was rejected (the power of the test) are between 0 and 1. Other conditions remain the same, the power and effect size are positive associated. The greater the difference between the true and estimated values of the population parameters, the greater the power. It can also be said that the larger the effect size, the greater the efficacy.
```{r True_mean-power plot}
n_obs_reject_df =
sim_results_df %>% 
  group_by(true_mean) %>% 
  filter(test_result == "reject H0") %>%
  summarize(
    n_obs_reject = n()) %>%
  mutate(percent = n_obs_reject / 5000) %>%
  ggplot(aes(x = true_mean, y = percent)) +
  geom_point(size = 1.5) +
  geom_line() +
  labs(
    title = "True_mean-power plot",
    x = "True mean",
    y = "The power of the test") +
  theme(legend.position = "bottom") +
  theme(axis.text.x = element_text(hjust = 1, vjust = .5),
        plot.title = element_text(hjust = 0.5))

n_obs_reject_df
```

